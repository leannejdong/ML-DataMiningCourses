%\documentclass[12pt, answers]{exam}
\documentclass[12pt, answers]{exam}
%\setlength{\textheight}{5in}
%\usepackage[textwidth=9in,centering]{geometry}
\usepackage[a4paper, margin=.9in]{geometry}
\usepackage{econometrics}
\usepackage{lmodern}
\usepackage{amssymb,amsmath,amsfonts,amsthm,enumerate,enumitem}
\usepackage{setspace, graphicx, multirow, dcolumn, caption}
\usepackage{bm}


% \usepackage{etoolbox}
% \AtEndEnvironment{question}{\bigskip\bigskip}

% \usepackage{etoolbox}
% \makeatletter
%  \patchcmd\@setheadheight{\endgroup}{\global\vsize=\textheight\endgroup}{}{}
% \makeatother


% Notation
\def\eps{\varepsilon}
\def\var{\text{Var}}
\def\cov{\text{Cov}}
\def\corr{\text{Corr}}
\def\Ybar{\overline{Y}}
\def\Xbar{\overline{X}}
\def\ybar{\overline{y}}
\def\xbar{\overline{x}}
\def\Yhat{\widehat{Y}}
\def\yhat{\widehat{y}}
\def\fhat{\widehat{f}}
\def\bhat{\widehat{\beta}}
\def\betahat{\widehat{\vbeta}}
\def\MSE{\text{MSE}}
\def\SE{\text{SE}}
\def\train{\mathcal{T}}
\def\data{\mathcal{D}}
\def\model{\mathcal{M}}
\def\hvtheta{\widehat{\vtheta}}
\def\({\left(}
\def\){\right)}

\def\v{\boldsymbol}

%\setlength{\parindent}{0pt}
\onehalfspace

%\renewcommand{\labelitemi}{1.}

%\bracketedpoints
\pointpoints{mark}{marks}
% \qformat{\textbf{Problem \thequestion: \\ \hspace{10pt} \\}}


\qformat{
    \textbf{Question \thequestion}
    \hfill
    \vrule depth 20pt width 0pt % Large depth to make space
}

\begin{document}

\begin{center}
{\Large \textbf{QBUS6810} \\\textbf{Statistical Learning and Data Mining}}\\ \bigskip \large{Tutorial 6 (Written Problems)}
%{\Large \textbf{QBUS6810} \\\textbf{Statistical Learning and Data Mining}}\\ \bigskip \large{Tutorial 5 (Written Exercises)}
\end{center}

%\bigskip

%\noindent \textbf{Important note:} some of these solutions are abbreviated. In the exam, you need to show your work in full detail. Write and justify every step of your reasoning. You should leave no doubt to the examiner that you understand what you are doing, as opposed to reciting material from memory.

%\bigskip

\begin{questions}

%\bigskip

\question

%Recall the additive error model:
%\[Y=f(X)+\eps,\]
%where $\eps$ is i.i.d. with $E(\eps)=0$ and $\var(\eps)=\sigma^2$ (for simplicity, we will treat~$X$ as independent of~$\eps$).

Show that the OLS estimator is unbiased, i.e., derive the following:
\begin{align*}
E\widehat{\vbeta}=\vbeta.
\end{align*}
Treat the $x$ values as fixed (i.e. non-random) and use the formula for the OLS estimator.\vspace{12pt}


\begin{solution}
Recall that the expected value of the error terms in the MLR model is zero.  We will make use of the following formulas (and all the corresponding notation) from Lecture 3:
\begin{align*}
\widehat{\vbeta}&=(\mX^T\mX)^{-1}\mX^T\vy\qquad\text{and}\\
\vy&=\mX\vbeta+\v\eps.
\end{align*}
In the following expected value calculations, non-random matrixes are treated as constants, which we can be factored out of the expected values.  We have:
\begin{align*}
E\widehat{\vbeta}&=E\Big[(\mX^T\mX)^{-1}\mX^T\vy\Big]\\
&=E\Big[(\mX^T\mX)^{-1}\mX^T(\mX\vbeta+\v\eps)\Big]\\
&=E\Big[(\mX^T\mX)^{-1}\mX^T\mX\vbeta\Big]+E\Big[(\mX^T\mX)^{-1}\mX^T\v\eps\Big]\\
&=(\mX^T\mX)^{-1}(\mX^T\mX)\vbeta+(\mX^T\mX)^{-1}\mX^TE\v\eps\\
&=\vbeta.
\end{align*}

\end{solution}



\clearpage
\question

%Recall the additive error model:
%\[Y=f(X)+\eps,\]
%where $\eps$ is i.i.d. with $E(\eps)=0$ and $\var(\eps)=\sigma^2$ (for simplicity, we will treat~$X$ as independent of~$\eps$).


Consider a method for learning the true regression function, $f$, in the additive error model.  It is known (this will be further discussed in the lectures later in the semester) that the expected value of the amount by which the training MSE underestimates the corresponding test MSE is given by
\begin{align}
\label{eq1}
\frac{2}{n}\sum_{i=1}^{n}\cov(\widehat{Y}_i,Y_i),
\end{align}
where $Y_i$ are the response values in the training data, and $\widehat{Y}_i=\widehat{f}(\vx_i)$ are the corresponding fitted values of the learning method.

\medskip


Show that the quantity in display~(\ref{eq1}) equals $2\sigma^2/k$ for the k-nearest neighbours regression method. Thus, the training error underestimates the test error by the largest amount in the case $k=1$, in which the training data is fitted perfectly.

Treat the $x$ values as fixed (i.e. non-random).   \vspace{12pt}

\begin{solution}
Focus on a particular~$i$ and let $\vx_{i_1},\vx_{i_2},...,\vx_{i_k}$ be the $k$ closest neighbors of $\vx_i$ in the training set (i.e. the $k$ points in $\{\vx_1,\vx_2,...,\vx_n\}$ that are the closest to $\vx_i$).  Note that $\vx_i$ is itself one of these~$k$ points.  Also note that random variables $Y_i$ and $Y_{i_j}$ are independent (and thus, uncorrelated) for $i\ne i_j$.  We have:
\begin{align*}
\cov(Y_i, \widehat{Y}_i)&=\cov\(Y_i, \frac1k[Y_{i_1}+Y_{i_2}+...+Y_{i_k}] \)\\
&=\cov\(Y_i, \frac{Y_{i_1}}k\)+\cov\(Y_i, \frac{Y_{i_2}}k\)+...+\cov\(Y_i, \frac{Y_{i_k}}k\)\\
&=\frac1k\cov\(Y_i, {Y_{i_1}}\)+\frac1k\cov\(Y_i, {Y_{i_2}}\)+...+\frac1k\cov\(Y_i, {Y_{i_k}}\)\\
&=\frac1k\cov\(Y_i, Y_i\)\\
&=\frac1k\var\(Y_i\)\\
&=\frac{\sigma^2}k.
\end{align*}

\medskip

Therefore,
\begin{align*}
\frac{2}{n}\sum_{i=1}^{n}\cov(\widehat{Y}_i,Y_i)=\frac{2}{n}\sum_{i=1}^{n}\frac{\sigma^2}k = \frac{2}{n} n\frac{\sigma^2}k= \frac{2\sigma^2}k.
\end{align*}

\end{solution}


\bigskip
\medskip

\question

Let $y_1,\ldots,y_n$ be a sample from a distribution with the density function $p(y;\theta)=\theta y^{\theta-1}$ for $0<y<1$, where $\theta>0$.

Find~$\widehat{\theta}$, the maximum likelihood estimator of~$\theta$.

Compute~$\widehat{\theta}$ for the sample $y_1=0.35$, $y_2=0.28$, $y_3=0.91$.

\begin{solution}
The likelihood function is
\begin{align*}
\ell(\theta)&=p(y_1;\theta)p(y_2;\theta)\ldots p(y_n;\theta)\\
&=\prod_{i=1}^n \theta y^{\theta-1}_i\\
&=\theta^n \prod_{i=1}^n y^{\theta-1}_i.
\end{align*}
Taking the natural log:
\begin{align*}
L(\theta)=\log(\ell(\theta))&=n \log(\theta)+(\theta-1)\sum_{i=1}^n \log(y_i).
\end{align*}
The first derivative is
\begin{align*}
\frac{d L(\theta)}{d \theta}=\frac{n}{\theta}+\sum_{i=1}^n \log(y_i).
\end{align*}
The first derivative is zero at~$\widehat{\theta}$:
\begin{align*}
\frac{n}{\widehat{\theta}}+\sum_{i=1}^n \log(y_i)=0.
\end{align*}
Thus,
\begin{align*}
\widehat{\theta}=\frac{-n}{\sum_{i=1}^n \log(y_i)}.
\end{align*}

\medskip

For the sample $0.35, 0.28, 0.99$, we have:
\begin{equation*}
\widehat{\theta}=\frac{-3}{\log(0.35)+\log(0.28)+\log(0.91)}=1.24.
\end{equation*}


\end{solution}


\clearpage
\question

Consider the following penalized least-squares estimator, called the \textit{Ridge regression estimator} (to be discussed in Lecture 6):
\begin{align*}
\betahat_{\text{ridge}} &= \underset{\vbeta}{\text{argmin}}\, \left\{ \sum_{i=1}^n\(y_i-\beta_0-\sum_{j=1}^{p}\beta_jx_{ij}\)^2 + \lambda \sum_{j=1}^{p}\beta_j^2\right\}
\end{align*}
Note that OLS is a special case of Ridge, corresponding to $\lambda=0$.



Show that if we set $\lambda=\sigma^2/\tau^2$, the ridge regression estimator is the posterior mode (i.e. the MAP estimator) in a Gaussian linear regression model with the prior on the regression coefficients under which $\beta_j$ are independent $N(0,\tau^2)$, for $j=1,...,p$.  Here we are not putting an informative prior on the intercept $\beta_0$ {\footnotesize(this is equivalent to using a flat prior density for $\beta_0$, i.e., a density that is proportional to the constant~$1$)}.



\bigskip

\begin{solution}

The following derivation of the posterior density is almost identical to the one used at the end of Lecture 5, except there is no $\beta_0$ component in the prior this time.

Note that random variables $Y_i$ are independent $N\(\beta_0+\sum_{j=1}^{p}\beta_jx_{ij}\,,\,\sigma^2\)$.  Using the symbol $\propto$ to denote ``proportional to'' and leaving out multiplicative constants, we have:
\begin{align*}
p(\vy |\vbeta) & \propto \prod_{i=1}^n\exp\left\{-\frac{1}{2\sigma^2}\(y_i-\beta_0-\sum_{j=1}^{p}\beta_jx_{ij}\)^2\right\}\\
&\propto \exp\left\{-\frac{1}{2\sigma^2}\sum_{i=1}^n\(y_i-\beta_0-\sum_{j=1}^{p}\beta_jx_{ij}\)^2\right\}.
\end{align*}
Similarly, the prior satisfies
\begin{align*}
p(\vbeta)& \propto \prod_{j=1}^p\exp\left\{-\frac{\beta_j^2}{2\tau^2}\right\}\\
& \propto \exp\left\{-\frac{1}{2\tau^2}\sum_{j=0}^{p}\beta_j^2\right\}.
\end{align*}

Hence, the posterior density has the form
\begin{eqnarray*}
p(\vbeta|\vy) & \propto & \exp\left\{-\frac{1}{2\sigma^2}\sum_{i=1}^n(y_i-\beta_0-\sum_{j=1}^{p}\beta_jx_{ij})^2\right\}\times \exp\left\{-\frac{1}{2\tau^2}\sum_{j=1}^{p}\beta_j^2\right\}\\
&&\\
&\propto& \exp\left\{-\frac{1}{2\sigma^2}\left[\sum_{i=1}^n(y_i-\beta_0-\sum_{j=1}^{p}\beta_jx_{ij})^2+\frac{\sigma^2}{\tau^2}\sum_{j=1}^{p}\beta_j^2\right]\right\}.
\end{eqnarray*}

Recall that the MAP estimator is the maximizer of both the posterior density and the log-posterior density.  We will work with the log-posterior for convenience.  Because $\lambda=\sigma^2/\tau^2$, the logarithm of the posterior density is:
\begin{equation*}
\log\left[ p(\vbeta|\vy)\right]=-\frac{1}{2\sigma^2}\left[\sum_{i=1}^n(y_i-\beta_0-\sum_{j=1}^{p}\beta_jx_{ij})^2+\lambda\sum_{j=1}^{p}\beta_j^2\right]+\text{constant},
\end{equation*}
where the ``constant'' comes from taking the $\log$ of the multiplicative factors that were left out in the expressions above.

Thus, the relevant part of the log-posterior consists of the ridge objective function times a negative multiplier. Hence, maximising the log-posterior is equivalent to \textit{minimising} the ridge objective function.  It follows that the MAP estimator is equivalent to the ridge estimator.

\end{solution}

\end{questions}

\end{document} 